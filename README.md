<div align="center">
  <p align="center">
    ğŸ¦ <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ 
    ğŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ 
    ğŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ 
    ğŸ“™ <a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python">Hands-on GNN</a>
  </p>
</div>

---

I'm a Machine Learning Scientist interested in Large Language Models (LLMs) and Graph Neural Networks (GNNs).

## ğŸ’¼ Projects

* [The LLM Course](https://github.com/mlabonne/llm-course): A popular curated list of resources to get into LLMs (>15k â­).
* [Hands-on GNNs](https://github.com/mlabonne/llm-course): My book about graph neural networks published by Packt (all the code is open source).
* [LLM AutoEval](https://github.com/mlabonne/llm-autoeval): Automatically evaluate your LLMs in Google Colab.

## ğŸ¤— Models

* [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8): The first Mixture of Experts with phi-2 models.
* [Beyonder](https://huggingface.co/mlabonne/Beyonder-4x7B-v2): Mixture of Experts with four excellent fine-tuned Mistral-7b models.
* [NeuralMarcoro14](https://huggingface.co/mlabonne/NeuralMarcoro14-7B): The best general-purpose 7B model when it was released.
* [NeuralHermes](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B): Most successful DPO fine-tuned version of OpenHermes when it was released.
